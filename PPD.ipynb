{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84d30f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6bb45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify file path of raw data\n",
    "path = 'C:/Users/33769/OneDrive/Bureau/Learning/2. Python/2. Dash/PPD/'\n",
    "file_path = path + 'pp-complete.csv'\n",
    "\n",
    "# Define column names\n",
    "column_names = ['transaction_id','price', 'transfer_date', 'postcode', 'property_type', 'new_build_flag', 'duration', 'primary_address', 'secondary_address', 'street', 'locality', 'city', 'district', 'county', 'ppd_category', 'record_status']\n",
    "\n",
    "# Specify data types for each column\n",
    "dtypes = dict.fromkeys(column_names, 'string')\n",
    "dtypes.update({'price': 'int64'})\n",
    "\n",
    "# Define columns we don't want\n",
    "usecols = [col for col in column_names if col != 'transaction_id']\n",
    "\n",
    "# Read CSV using Dask with specified data types and parse_dates\n",
    "data = dd.read_csv(file_path,header=None, names=column_names,dtype=dtypes,usecols=usecols)\n",
    "\n",
    "# Compute to get a Pandas DataFrame\n",
    "df = data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53a8a746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>transfer_date</th>\n",
       "      <th>postcode</th>\n",
       "      <th>property_type</th>\n",
       "      <th>new_build_flag</th>\n",
       "      <th>duration</th>\n",
       "      <th>primary_address</th>\n",
       "      <th>secondary_address</th>\n",
       "      <th>street</th>\n",
       "      <th>locality</th>\n",
       "      <th>city</th>\n",
       "      <th>district</th>\n",
       "      <th>county</th>\n",
       "      <th>ppd_category</th>\n",
       "      <th>record_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42000</td>\n",
       "      <td>1995-12-21 00:00</td>\n",
       "      <td>NE4 9DN</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>MATFEN PLACE</td>\n",
       "      <td>FENHAM</td>\n",
       "      <td>NEWCASTLE UPON TYNE</td>\n",
       "      <td>NEWCASTLE UPON TYNE</td>\n",
       "      <td>TYNE AND WEAR</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>95000</td>\n",
       "      <td>1995-03-03 00:00</td>\n",
       "      <td>RM16 4UR</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>30</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>HEATH ROAD</td>\n",
       "      <td>GRAYS</td>\n",
       "      <td>GRAYS</td>\n",
       "      <td>THURROCK</td>\n",
       "      <td>THURROCK</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74950</td>\n",
       "      <td>1995-10-03 00:00</td>\n",
       "      <td>CW10 9ES</td>\n",
       "      <td>D</td>\n",
       "      <td>Y</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>SHROPSHIRE CLOSE</td>\n",
       "      <td>MIDDLEWICH</td>\n",
       "      <td>MIDDLEWICH</td>\n",
       "      <td>CONGLETON</td>\n",
       "      <td>CHESHIRE</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43500</td>\n",
       "      <td>1995-11-14 00:00</td>\n",
       "      <td>TS23 3LA</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>19</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>SLEDMERE CLOSE</td>\n",
       "      <td>BILLINGHAM</td>\n",
       "      <td>BILLINGHAM</td>\n",
       "      <td>STOCKTON-ON-TEES</td>\n",
       "      <td>STOCKTON-ON-TEES</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63000</td>\n",
       "      <td>1995-09-08 00:00</td>\n",
       "      <td>CA25 5QH</td>\n",
       "      <td>S</td>\n",
       "      <td>N</td>\n",
       "      <td>F</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CROSSINGS CLOSE</td>\n",
       "      <td>CLEATOR MOOR</td>\n",
       "      <td>CLEATOR MOOR</td>\n",
       "      <td>COPELAND</td>\n",
       "      <td>CUMBRIA</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price     transfer_date  postcode property_type new_build_flag duration  \\\n",
       "0  42000  1995-12-21 00:00   NE4 9DN             S              N        F   \n",
       "1  95000  1995-03-03 00:00  RM16 4UR             S              N        F   \n",
       "2  74950  1995-10-03 00:00  CW10 9ES             D              Y        F   \n",
       "3  43500  1995-11-14 00:00  TS23 3LA             S              N        F   \n",
       "4  63000  1995-09-08 00:00  CA25 5QH             S              N        F   \n",
       "\n",
       "  primary_address secondary_address            street      locality  \\\n",
       "0               8              <NA>      MATFEN PLACE        FENHAM   \n",
       "1              30              <NA>        HEATH ROAD         GRAYS   \n",
       "2              15              <NA>  SHROPSHIRE CLOSE    MIDDLEWICH   \n",
       "3              19              <NA>    SLEDMERE CLOSE    BILLINGHAM   \n",
       "4               8              <NA>   CROSSINGS CLOSE  CLEATOR MOOR   \n",
       "\n",
       "                  city             district            county ppd_category  \\\n",
       "0  NEWCASTLE UPON TYNE  NEWCASTLE UPON TYNE     TYNE AND WEAR            A   \n",
       "1                GRAYS             THURROCK          THURROCK            A   \n",
       "2           MIDDLEWICH            CONGLETON          CHESHIRE            A   \n",
       "3           BILLINGHAM     STOCKTON-ON-TEES  STOCKTON-ON-TEES            A   \n",
       "4         CLEATOR MOOR             COPELAND           CUMBRIA            A   \n",
       "\n",
       "  record_status  \n",
       "0             A  \n",
       "1             A  \n",
       "2             A  \n",
       "3             A  \n",
       "4             A  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29ddc48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transfer_date'] = dd.to_datetime(df['transfer_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bc4d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price                          int64\n",
       "transfer_date         datetime64[ns]\n",
       "postcode             string[pyarrow]\n",
       "property_type        string[pyarrow]\n",
       "new_build_flag       string[pyarrow]\n",
       "duration             string[pyarrow]\n",
       "primary_address      string[pyarrow]\n",
       "secondary_address    string[pyarrow]\n",
       "street               string[pyarrow]\n",
       "locality             string[pyarrow]\n",
       "city                 string[pyarrow]\n",
       "district             string[pyarrow]\n",
       "county               string[pyarrow]\n",
       "ppd_category         string[pyarrow]\n",
       "record_status        string[pyarrow]\n",
       "dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c195722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer uniquement les années 2024 et 2025\n",
    "df = df[df['transfer_date'].dt.year.isin(range(2020, 2026))]\n",
    "\n",
    "# Réinitialiser l'index proprement\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fbfaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Drop rows with no postcode\n",
    "df = df.dropna(subset=['postcode'])\n",
    "\n",
    "#Exclude property type Other (O)\n",
    "df = df.loc[df['property_type']!='O']\n",
    "\n",
    "# Sort by Date\n",
    "#df.sort_values(by=['transfer_date'], inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b29eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207785, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd527a7c",
   "metadata": {},
   "source": [
    "Add postcode latitude and longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the postcode CSV file\n",
    "postcodes_df = pd.read_csv('ukpostcodes.csv')\n",
    "\n",
    "# Convert df to dict for faster lookup\n",
    "postcodes = dict()\n",
    "for (postcode, latitude, longitude) in postcodes_df[['postcode', 'latitude', 'longitude']].values:postcodes[postcode] = [latitude, longitude]\n",
    "\n",
    "# Apply the mapping to create the new column\n",
    "df['postcode_lat_long'] = df['postcode'].map(postcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add postcode sectors\n",
    "df['postcode_sector'] =  df['postcode'].apply(lambda x: x[:x.find(' ')+2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8aeef",
   "metadata": {},
   "source": [
    "Add region column as defined by GeoJSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20495921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load regions by postcodes\n",
    "regions_df = pd.read_csv('postcode_areas.csv', usecols=['postcode_prefix', 'region'])\n",
    "\n",
    "# Create a dictionary from the DataFrame\n",
    "postcode_region_dict = dict(zip(regions_df['postcode_prefix'], regions_df['region']))\n",
    "\n",
    "# Add postcode prefix column to main df\n",
    "df['postcode_prefix'] = df['postcode'].str.extract(r'^([A-Z]+)')\n",
    "\n",
    "# Map the postcode prefix to the region using the dictionary\n",
    "df['region'] = df['postcode_prefix'].map(postcode_region_dict)\n",
    "\n",
    "# Drop the intermediate 'postcode_prefix' column\n",
    "df.drop(columns=['postcode_prefix'], inplace=True)\n",
    "\n",
    "#Exclude sales in Scotland as limited data available for this region\n",
    "df = df.loc[df['region']!='Scotland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b886bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add year column\n",
    "# Extract the year from 'transfer_date'\n",
    "df['year'] = df['transfer_date'].dt.year\n",
    "\n",
    "# Extract the month from 'transfer_date'\n",
    "df['month'] = df['transfer_date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61077238",
   "metadata": {},
   "source": [
    "Create GeoJSON files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94603160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\33769\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyogrio\\geopandas.py:710: UserWarning: 'crs' was not provided.  The output dataset will not have projection information defined and may not be usable in other systems.\n",
      "  write(\n"
     ]
    }
   ],
   "source": [
    "# Create GeoJSON file for all sectors using Shapefil\n",
    "myshpfile = gpd.read_file(path+'shapefiles/Sectors.shp')\n",
    "myshpfile.to_file(path+'geo_json/all_sectors.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee917f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the postcode prefixes CSV\n",
    "postcode_prefixes_df = pd.read_csv('postcode_areas.csv')\n",
    "\n",
    "# Load the GeoJSON file containing postcode sector data\n",
    "geojson_path = 'geo_json/all_sectors.geojson'\n",
    "postcode_sectors_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Create an empty dictionary to store GeoDataFrames for each region\n",
    "region_geojson_dict = {}\n",
    "\n",
    "# Extract postcode prefix from 'name' property\n",
    "postcode_sectors_gdf['postcode_prefix'] = pd.Series(postcode_sectors_gdf['name']).str.extract(r'^([A-Z]+)')\n",
    "\n",
    "# Merge with postcode prefixes DataFrame to get regions\n",
    "merged_df = pd.merge(postcode_sectors_gdf, postcode_prefixes_df, how='left', on='postcode_prefix')\n",
    "\n",
    "# Create a dictionary to store GeoDataFrames for each region\n",
    "region_geojson_dict = {}\n",
    "\n",
    "# Iterate over unique regions\n",
    "for region in merged_df['region'].unique():\n",
    "    # Filter DataFrame for the current region\n",
    "    region_df = merged_df[merged_df['region'] == region].copy()\n",
    "    \n",
    "    # Create a GeoDataFrame for the current region\n",
    "    region_gdf = gpd.GeoDataFrame(region_df, geometry='geometry', crs=postcode_sectors_gdf.crs)\n",
    "    \n",
    "    # Store the GeoDataFrame in the dictionary under the region key\n",
    "    region_geojson_dict[region] = region_gdf\n",
    "\n",
    "# Now, region_geojson_dict contains GeoDataFrames for each region\n",
    "\n",
    "for region, region_gdf in region_geojson_dict.items():\n",
    "    region_geojson_path = f'geo_json/regions/{region}_postcode_sectors.geojson'\n",
    "    region_gdf.to_file(region_geojson_path, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de21eb",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910fe426",
   "metadata": {},
   "source": [
    "Average Price by year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7f4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region, postcode_sector, and year, then calculate the rounded average price and count the number of sales\n",
    "grouped_df = df.groupby(['region', 'postcode_sector', 'year']).agg({'price': ['mean', 'count']}).reset_index()\n",
    "grouped_df.columns = ['region', 'postcode_sector', 'year','avg_price', 'volume']\n",
    "\n",
    "# Round the average price to the nearest thousand and convert to integers\n",
    "grouped_df['avg_price'] = grouped_df['avg_price'].round(-3).astype(int)\n",
    "\n",
    "# Loop through unique years and save CSV for each year\n",
    "for year in grouped_df['year'].unique():\n",
    "    # Filter DataFrame for the current year\n",
    "    year_df = grouped_df[grouped_df['year'] == year]\n",
    "    \n",
    "    # Create the CSV file path for the current year\n",
    "    csv_file_path = (f'processed_data/average_price_by_year/region_data_{year}.csv')\n",
    "    \n",
    "    # Save the filtered DataFrame to CSV\n",
    "    year_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293e5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16382 entries, 0 to 16381\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   region           16382 non-null  object\n",
      " 1   postcode_sector  16382 non-null  object\n",
      " 2   year             16382 non-null  int32 \n",
      " 3   avg_price        16382 non-null  int64 \n",
      " 4   volume           16382 non-null  int64 \n",
      "dtypes: int32(1), int64(2), object(2)\n",
      "memory usage: 576.1+ KB\n"
     ]
    }
   ],
   "source": [
    "grouped_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce14ed5",
   "metadata": {},
   "source": [
    "Avg price for each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79998172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by year, region, property_type, and calculate rounded average price\n",
    "region_grouped_df = df.groupby(['year', 'region', 'property_type']).agg(avg_price=('price', 'mean')).round({'avg_price': -3}).astype({'avg_price': int}).reset_index()\n",
    "\n",
    "# Calculate sales volume for each group\n",
    "region_grouped_df['volume'] = df.groupby(['year', 'region', 'property_type']).size().reset_index(name='volume')['volume']\n",
    "\n",
    "# Save the grouped data to a single CSV file\n",
    "region_grouped_df.to_csv('processed_data/region_avg_price/region_avg_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a752c9",
   "metadata": {},
   "source": [
    "Percentage delta of average price (year on year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05615a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region and postcode_sector, then calculate the percentage delta\n",
    "delta_df = grouped_df.copy()  # Create a new DataFrame to store the results\n",
    "delta_df['delta'] = grouped_df.groupby(['region', 'postcode_sector'])['avg_price'].pct_change() * 100\n",
    "\n",
    "# Set the percentage change for the first year to 0\n",
    "delta_df.loc[delta_df['year'] == delta_df['year'].min(), 'delta'] = 0\n",
    "\n",
    "# Drop any rows with a null delta\n",
    "delta_df = delta_df.dropna(subset=['delta'])\n",
    "\n",
    "# Round values to the nearest integer\n",
    "delta_df['delta'] = delta_df['delta'].round().astype(int)\n",
    "\n",
    "# Get list of years from df\n",
    "unique_years = delta_df['year'].unique()\n",
    "\n",
    "# Iterate over each year and create a CSV file\n",
    "for year in unique_years:\n",
    "    # Filter rows for the current year\n",
    "    year_df = delta_df[delta_df['year'] == year]\n",
    "\n",
    "    # Create a CSV file for the current year\n",
    "    csv_file_path = f'processed_data/avg_price_delta/avg_price_delta_{year}.csv'\n",
    "    year_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f3effb",
   "metadata": {},
   "source": [
    "Volume by month each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af66b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by region, postcode_sector, year, and month, then calculate the count of sales\n",
    "volume_df = df.groupby(['region', 'postcode_sector', 'year', 'month']).agg({'price': 'count'}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "volume_df.columns = ['region', 'postcode_sector', 'year', 'month', 'volume']\n",
    "\n",
    "# Loop through unique years\n",
    "for year in volume_df['year'].unique():\n",
    "    # Filter DataFrame for the current year\n",
    "    year_df = volume_df[volume_df['year'] == year]\n",
    "    \n",
    "    # Group by region and month, then calculate the total volume\n",
    "    total_volume_df = year_df.groupby(['region', 'month']).agg({'volume': 'sum'}).reset_index()\n",
    "    \n",
    "    # Create the CSV file path for the current year and save the DataFrame\n",
    "    csv_file_path = f'processed_data/volume_by_year/region_total_volume_{year}.csv'\n",
    "    total_volume_df.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
